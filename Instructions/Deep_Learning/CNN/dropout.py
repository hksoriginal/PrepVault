dropout_markdown = r"""
# Dropout in Deep Learning

Dropout is a regularization technique used in neural networks to prevent overfitting. It was introduced by Geoffrey Hinton et al. in 2014. The main idea is to randomly set a fraction of the neurons to zero during training, which forces the network to learn more robust features.

## How Dropout Works

During training, for each training instance, a random subset of neurons is "dropped out" (i.e., set to zero). This means that the neurons do not contribute to the forward pass and do not participate in the backpropagation for that specific training instance.

### Mathematical Representation

Let $$( x )$$ be the input to a layer and $$( W )$$ be the weights of that layer. The output $$( y )$$ before applying dropout can be represented as:

$$
y = f(W \cdot x + b)
$$

where:
- $$( f )$$ is the activation function,
- $$( b )$$ is the bias.

With dropout, we introduce a mask $$( M )$$ that randomly selects which neurons to keep. If we keep a fraction $$( p )$$ of the neurons, the mask $$( M )$$ can be defined as:

$$
M_i = 
\begin{cases} 
1 & \text{with probability } p \\ 
0 & \text{with probability } 1 - p 
\end{cases}
$$

The output with dropout applied becomes:

$$
y' = M \odot y
$$

where $$( \odot )$$ denotes the element-wise multiplication.

### Training Phase

During training, dropout is applied. If the dropout rate is $$( r )$$, then the probability of keeping a neuron is $$( p = 1 - r )$$.

### Inference Phase

During inference (i.e., when making predictions), dropout is not applied. Instead, the outputs are scaled to account for the neurons that were dropped during training. The output becomes:

$$
y'' = \frac{y}{p}
$$

This scaling ensures that the expected value of the output remains the same during training and inference.

## Benefits of Dropout

1. **Reduces Overfitting**: By randomly dropping neurons, the model learns to rely on different subsets of neurons, making it more generalizable.
2. **Improves Robustness**: The model becomes more robust to the presence of noise in the input data.
3. **Acts as an Ensemble Method**: Each forward pass can be viewed as training a different neural network, thus combining the predictions of many networks.

## Implementation

Dropout is commonly implemented in deep learning frameworks like TensorFlow and PyTorch. Hereâ€™s an example of how to use dropout in PyTorch:

```python
import torch
import torch.nn as nn

class SimpleNN(nn.Module):
    def __init__(self):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.dropout = nn.Dropout(p=0.5)  # Dropout with 50% probability
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)  # Apply dropout
        x = self.fc2(x)
        return x
```

## Dropout Layer 
```python
# Load and preprocess the CIFAR-10 dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize to [0, 1]

# Create the CNN model
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.5),  # Dropout layer with a rate of 0.5 (50%)
    layers.Dense(10, activation='softmax')  # Output layer for 10 classes
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))

```

"""
