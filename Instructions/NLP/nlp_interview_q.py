NLP_INTERVIEW_Q = r'''
# Natural Language Processing (NLP) Interview Questions and Answers

## 1. What is NLP?
**Answer:** NLP (Natural Language Processing) is a branch of AI that enables computers to understand, interpret, and respond to human language.

## 2. What are the major tasks of NLP?
**Answer:** NLP tasks include tokenization, stemming, lemmatization, POS tagging, named entity recognition (NER), syntactic parsing, machine translation, sentiment analysis, and text summarization.

## 3. What is tokenization in NLP?
**Answer:** Tokenization is the process of splitting text into smaller units, such as words or sentences.

## 4. What is stemming?
**Answer:** Stemming reduces words to their root form (e.g., "running" -> "run").

## 5. What is lemmatization?
**Answer:** Lemmatization converts words to their base form using vocabulary and morphological analysis (e.g., "better" -> "good").

## 6. What is POS tagging?
**Answer:** POS (Part-of-Speech) tagging assigns grammatical categories to words, such as noun, verb, or adjective.

## 7. What is Named Entity Recognition (NER)?
**Answer:** NER identifies and classifies named entities like persons, locations, and organizations in text.

## 8. What is the difference between N-grams and bag-of-words?
**Answer:** N-grams consider word sequences, while bag-of-words represents text as an unordered collection of words.

## 9. What is word embedding?
**Answer:** Word embedding is a vector representation of words that captures their meanings and relationships (e.g., Word2Vec, GloVe, FastText).

## 10. What is TF-IDF?
**Answer:** TF-IDF (Term Frequency-Inverse Document Frequency) is a weighting technique that reflects how important a word is in a document relative to a collection of documents.

## 11. What is the difference between stemming and lemmatization?
**Answer:** Stemming is a heuristic process that chops words, while lemmatization uses linguistic knowledge to obtain root forms.

## 12. What is cosine similarity?
**Answer:** Cosine similarity measures the similarity between two vectors by computing the cosine of the angle between them.

## 13. What are stop words?
**Answer:** Stop words are common words (e.g., "the", "is", "in") that are usually removed during text preprocessing.

## 14. What is perplexity in NLP?
**Answer:** Perplexity is a measure of a language model's uncertainty in predicting the next word.

## 15. What is a Transformer model?
**Answer:** A Transformer is a deep learning model architecture that uses self-attention mechanisms for NLP tasks (e.g., BERT, GPT).

## 16. What is BERT?
**Answer:** BERT (Bidirectional Encoder Representations from Transformers) is a Transformer-based model that learns contextual relationships from text bidirectionally.

## 17. What is GPT?
**Answer:** GPT (Generative Pre-trained Transformer) is an autoregressive language model trained to generate human-like text.

## 18. What is Seq2Seq?
**Answer:** Sequence-to-Sequence (Seq2Seq) is an NLP model used for tasks like machine translation and summarization.

## 19. What is attention in NLP?
**Answer:** Attention mechanisms allow models to focus on relevant parts of input sequences.

## 20. What is self-attention?
**Answer:** Self-attention calculates the relevance of each word in a sentence to all other words.

## 21. What is BLEU score?
**Answer:** BLEU (Bilingual Evaluation Understudy) is a metric for evaluating machine translation quality by comparing generated and reference translations.

## 22. What is perplexity in language models?
**Answer:** Perplexity quantifies how well a probabilistic model predicts a sample.

## 23. What is word sense disambiguation?
**Answer:** It determines the correct meaning of a word in context.

## 24. What are pre-trained NLP models?
**Answer:** Pre-trained models (e.g., BERT, GPT, T5) are trained on large datasets and fine-tuned for specific tasks.

## 25. What is zero-shot learning in NLP?
**Answer:** Zero-shot learning enables a model to perform tasks without direct training.

## 26. What is transfer learning in NLP?
**Answer:** Transfer learning adapts a pre-trained model for new tasks.

## 27. What is LDA?
**Answer:** LDA (Latent Dirichlet Allocation) is a topic modeling technique that discovers topics in a document corpus.

## 28. What is perplexity in LDA?
**Answer:** Lower perplexity indicates a better topic model.

## 29. What is Named Entity Recognition used for?
**Answer:** NER is used for information extraction, chatbots, and document classification.

## 30. What is dependency parsing?
**Answer:** Dependency parsing analyzes grammatical relationships in a sentence.

## 31. What are some common NLP libraries?
**Answer:** NLTK, spaCy, Hugging Face Transformers, Gensim, and OpenNLP.

## 32. What is fastText?
**Answer:** fastText is a word embedding and text classification library by Facebook.

## 33. What is the role of an attention mask in NLP?
**Answer:** An attention mask prevents models from attending to padding tokens.

## 34. What is backpropagation through time (BPTT)?
**Answer:** BPTT is a training technique for RNNs that propagates errors backward over time steps.

## 35. How do you evaluate an NLP model?
**Answer:** Using metrics like accuracy, BLEU, ROUGE, and perplexity.

## 36. What is WordNet?
**Answer:** WordNet is a lexical database for the English language.

## 37. What is T5?
**Answer:** T5 (Text-to-Text Transfer Transformer) is a unified NLP model by Google.

## 38. What is RoBERTa?
**Answer:** RoBERTa is an improved version of BERT that removes the next sentence prediction task.

## 39. What is XLNet?
**Answer:** XLNet is a Transformer model that improves upon BERT using permutation-based training.

## 40. What is a bidirectional LSTM?
**Answer:** A bidirectional LSTM processes text forward and backward for better context understanding.

## 41. What are embeddings in NLP?
**Answer:** Embeddings are vector representations of words or phrases.

## 42. What is sentence embedding?
**Answer:** Sentence embeddings represent entire sentences in vector space.

## 43. What is spaCy?
**Answer:** spaCy is an NLP library optimized for efficiency and production use.

## 44. What is coreference resolution?
**Answer:** Coreference resolution identifies expressions that refer to the same entity.

## 45. What is a beam search?
**Answer:** Beam search is a heuristic search algorithm used in text generation.

## 46. What is contrastive learning in NLP?
**Answer:** Contrastive learning learns representations by distinguishing similar and dissimilar examples.

## 47. What is masked language modeling (MLM)?
**Answer:** MLM is used in BERT to predict missing words in sentences.

## 48. What is a chatbot?
**Answer:** A chatbot is an AI system that interacts with users using natural language.

## 49. What is syntactic parsing?
**Answer:** Syntactic parsing determines the grammatical structure of a sentence.

## 50. What is a neural machine translation (NMT)?
**Answer:** NMT is a deep learning-based approach for machine translation.


'''
